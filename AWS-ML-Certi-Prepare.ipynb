{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 -bucket management\n",
    "# lifecycle rule add- transform from standard to IA\n",
    "# standard\n",
    "# IA- Infrequence access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encryption\n",
    "# sse-s3\n",
    "# sse-kms(you can manage your own key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3 security\n",
    "\n",
    "# user based- IAM policy(give access to a speicific user)\n",
    "# resource based- bucket wide policy, policy-grant access/encryption\n",
    "\n",
    "# VPC endpoint gateway\n",
    "# virtual point cloud(allow traffic only go through vpc instead of public web)\n",
    "# make sure private service(sagemaker) can access s3\n",
    "\n",
    "# add Tags on s3 objects(more secury)\n",
    "# tag usage- give access/lifecyle management/aws Cloudwatch metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS Kinesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kinesis Streams\n",
    "\n",
    "# divided in shards: producers- shard1/shard2.../- consumers, manage scaling yourself\n",
    "# real time, provision in advance\n",
    "# data retention(save for 1 to 7 days)-replay capability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kinesis Analytics\n",
    "\n",
    "# perform real time analytics on streaming data using SQL\n",
    "# use case: streaming ETL select columns/continuous metric generation(live leadboard mobile game)/live alert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kinesis Firehose\n",
    "\n",
    "# data ingestion, load streams into S3/Redshift(data warehouse)/ElasticSearch/Splunk\n",
    "# fully managed service- no administration , near real time\n",
    "# data transformation through AWS Lambda(csv--json)\n",
    "# converted(AWS Glue)\n",
    "# automated scaling\n",
    "# no data storage\n",
    "\n",
    "# s3 buffer size 1MB/s- reach 1MB or each 60s send data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kinesis Video Streams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS Glue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glue is serverless\n",
    "# cralwer\n",
    "# generate database and schema(including folder partition) from s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Glue ETL service-underlying platform(Apache Spark)\n",
    "\n",
    "# Bundle Transformations\n",
    "# drop columns(null)/filter/Join tables/Map(add fields)\n",
    "# rename columns\n",
    "\n",
    "# Machine Learning Transformations\n",
    "# Identify duplicate in dataset\n",
    "\n",
    "# format conversion\n",
    "# json/csv/parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS Athena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3--Glue Catalog(generate db and schema)--Athena(Run SQL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS Data Store all service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redshift \n",
    "# data warehousing/SQL analytics/OLAP \n",
    "\n",
    "# RDS/Aurora\n",
    "# OLTP\n",
    "\n",
    "# DynamoDB\n",
    "# NoSQL-not only sql/ store ml model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data pipeline manage ec2 instances\n",
    "\n",
    "# ETL service, move data from one to another\n",
    "\n",
    "# move data from RDS/DynamoDB--S3--Sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Pipeline vs Glue\n",
    "\n",
    "# Glue ETL: run Apache Spark/Python based code, focus on ETL/ no worries about configuring, managing resources\n",
    "# created db, Data Catalog introduce data to --Athena/Redshift\n",
    "\n",
    "# Data Pipeline ETL: More control over environment, more control over environment, access to EC2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have a docker image works for batch(ex: clean up tasks)\n",
    "\n",
    "# non etl related(use case: financial service: after one day, post trade day analytics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS DMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# database migration service-replicated data from on-premises to cloud\n",
    "\n",
    "# move from source db to target db\n",
    "# ex local Oracle/SQL Server to Oralce/Aurora\n",
    "\n",
    "# no data transformation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS Step Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use to design/manage workflows\n",
    "# error handling(error happen at which step?)/audit\n",
    "\n",
    "# visulization\n",
    "# what happens first? then next step\n",
    "# ex: generate dataset:(Lambda)--model training(sagemaker)--save model)s3--batch transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time series data\n",
    "# trend + seasonality can both exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unbalance data\n",
    "# oversampling/undersampling/SMOTE/different metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature engineering\n",
    "\n",
    "# scaling\n",
    "# regression- less training time/if sclaing, weights can be used as to compare feature importance\n",
    "# tree- not affect\n",
    "# distance based clustering- affected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS Quicksight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# anomaly detection\n",
    "# forcasting\n",
    "# auto-narratives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS EMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# elastic map reduce\n",
    "# can be used to do processing data- normlizaing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EMR cluster\n",
    "# master node- single ec2 instance\n",
    "# core node - host HDFS data, compute\n",
    "# task node - not host data, only computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EMR storage\n",
    "\n",
    "# HDFS(default)-distribute/multiple copies on different instances-when instance shut down, data is gone\n",
    "# EMRFS- access S3 as if it were HDFS\n",
    "# Local files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hierarchy \n",
    "\n",
    "# MapReduce/Spark\n",
    "# Yarn-manage or allocate resources(cluster manager)\n",
    "# HDFS(instance/server)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS Sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# anomaly detection\n",
    "# sagemaker-random cut forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use xgboost as a framework\n",
    "# data processing and other your scripts can be added\n",
    "\n",
    "# use xgboost as built-in algorithm(more straight forward)\n",
    "# build xgboost container to construct an estimator and initiate a training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integration with Spark\n",
    "\n",
    "# pipeline: \n",
    "# preprocessing - Spark\n",
    "# model tuning/training/evaluation - Sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sagemaker autopilot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sagemaker model monitor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS High Level Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use AWS Lambda to automatically trigger something\n",
    "# e.g: data save to s3-trigger iamge analysis(AWS Rekognition) upon upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alexa workflow\n",
    "\n",
    "# AWS Transcribe - turn speech into text\n",
    "# AWS Lex - come back text as response or use Lambda to trigger other activites\n",
    "# AWS Polly - speak text back thru Alexa\n",
    "\n",
    "# Lex is the inner working of Alexa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jeff Bezos detector\n",
    "\n",
    "# Deeplens(set camera connected to rekoginition somewhere in Seattle)-Rekognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# People on the phone happy?\n",
    "# Transcibre - Comprehendz(Positive/Negative)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
